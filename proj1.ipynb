{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proj1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShainaBagri/ClassifyDocumentTopics/blob/main/proj1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu-zBj8yirhF",
        "outputId": "953afb0a-b466-444f-882f-c23ea4743a39"
      },
      "source": [
        "!wget 'http://users.csc.calpoly.edu/~foaad/proj1F21_files.zip'\n",
        "!unzip 'proj1F21_files.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 04:36:05--  http://users.csc.calpoly.edu/~foaad/proj1F21_files.zip\n",
            "Resolving users.csc.calpoly.edu (users.csc.calpoly.edu)... 129.65.128.20\n",
            "Connecting to users.csc.calpoly.edu (users.csc.calpoly.edu)|129.65.128.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 129006 (126K) [application/zip]\n",
            "Saving to: ‘proj1F21_files.zip’\n",
            "\n",
            "proj1F21_files.zip  100%[===================>] 125.98K   464KB/s    in 0.3s    \n",
            "\n",
            "2021-10-20 04:36:06 (464 KB/s) - ‘proj1F21_files.zip’ saved [129006/129006]\n",
            "\n",
            "Archive:  proj1F21_files.zip\n",
            "   creating: proj1F21_files/\n",
            "  inflating: proj1F21_files/proj1F21_1412_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_1412_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_1422_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_1422_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_1483_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_1483_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_1794_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_1794_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_1901_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_1901_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_2404_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_2404_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_2422_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_2422_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_2509_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_2509_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_2592_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_2592_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_2605_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_2605_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_2925_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_2925_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3061_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3061_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3363_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3363_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3453_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3453_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3490_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3490_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3509_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3509_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3550_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3550_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3618_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3618_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3647_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3647_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3928_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3928_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_3942_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_3942_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_4001_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_4001_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_4079_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_4079_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_6218_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_6218_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_6388_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_6388_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_6533_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_6533_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_6585_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_6585_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_6728_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_6728_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_6737_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_6737_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_8630_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_8630_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_8842_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_8842_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_8980_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_8980_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_9348_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_9348_B.html  \n",
            "  inflating: proj1F21_files/proj1F21_9578_A.html  \n",
            "  inflating: proj1F21_files/proj1F21_9578_B.html  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD5hswpu09YW",
        "outputId": "82dca831-ea33-4f1e-e681-bb395e945a38"
      },
      "source": [
        "!pip install html2text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: html2text in /usr/local/lib/python3.7/dist-packages (2020.1.16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsfKLCZA28NW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8007c441-a842-4b2f-ee7e-57642da598c8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# from nltk import PunktSentenceTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random \n",
        "import html2text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "random.seed(123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1V5EDI5ZXi"
      },
      "source": [
        "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "def tokenizeText(text):\n",
        "  goodParas = []\n",
        "  for para in text.split('\\n\\n\\n'):\n",
        "    sents = sent_tokenizer.tokenize(para.replace('\\n', ' ').strip())\n",
        "    goodSents = []\n",
        "    for sent in sents:\n",
        "      # TODO: lemmatize / convert contractions here?\n",
        "      words = [word.lower() for word in nltk.word_tokenize(sent)]\n",
        "      goodSents.append(words)\n",
        "    \n",
        "    goodParas.append(goodSents)\n",
        "      \n",
        "  return goodParas\n",
        "\n",
        "def extractFeaturesDoc(tokText):\n",
        "  def dummy(x):\n",
        "    return x\n",
        "  \n",
        "  features = {}\n",
        "  # tfidf\n",
        "  tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy, preprocessor=dummy, token_pattern=None, max_df=0.75)\n",
        "  features['tfidf'] = tfidf.fit_transform(tokText)\n",
        "\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HkMttbDh718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf821b4-9f87-438d-a96e-8b48fe1a0be6"
      },
      "source": [
        "dir = './proj1F21_files/'\n",
        "# docs = pd.DataFrame(columns=['author', 'subject', 'rawText'])\n",
        "docs = []\n",
        "for document in os.listdir(dir):\n",
        "  dict = {}\n",
        "  docName = document.split(\"_\")\n",
        "  dict['author'] = docName[1]\n",
        "  dict['subject'] = docName[2][0]\n",
        "  with open(dir + document) as f:\n",
        "    dict['htmlText'] = f.read()\n",
        "    dict['rawText'] = html2text.html2text(dict['htmlText'])\n",
        "    # tokenize \n",
        "    dict['tokenizedText'] = tokenizeText(dict['rawText']) \n",
        "\n",
        "  docs.append(dict)\n",
        "\n",
        "# split into training and test sets\n",
        "random.shuffle(docs)\n",
        "split = int(len(docs) *0.8)\n",
        "test_set = pd.DataFrame(docs[split:])\n",
        "training_set = pd.DataFrame(docs[:split])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(training_set['htmlText'][0])\n",
        "print(training_set['rawText'][0])\n",
        "print(training_set['tokenizedText'][0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
            "        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
            "<html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
            "<head>\n",
            "  <meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n",
            "</head>\n",
            "<body>\n",
            "<div style=\"width: 600px; margin: 20px auto; border: 1px solid #888; padding: 20px;\">\n",
            "  <p>One of my favorite people is my best friend Arya. She's one of those people that everybody gets along with. She's incredibly kind and patient with people. Her house was where we threw all the parties since her parents were very laidback. If I think back about high school, most of my best memories were at her house. We were the dynamic duo planners of our group. She was the eternal pacifist and is very welcoming. She'll always invite new people to hangouts and introduce to everybody and make sure they feel welcome.&nbsp;</p>\n",
            "<p>&nbsp;</p>\n",
            "<p>I think its important to have a friend who support you but also tells you when you're making a mistake. She's that friend for me. She's always been a huge supporter and also pushed me when I needed it. She's also never been afraid to call me out on my mistakes. In terms of mistakes, one thing that I appreciate a lot about her is that she's always trying to grow from her mistakes and is very introspective about her behavior.</p>\n",
            "<p>&nbsp;</p>\n",
            "<p>We had a fight a few years ago and our friendship became very surface level and strained. We reconnected again a few years later and we've become much strong friends because of it. We balance each other out now. She's a very nostalgic person and sometimes has a tendency to look at the past through rose-tinted lens and as a result that's taught me look at the past with both its highs and lows. I draw boundaries with people that I don't like and have no issue not interacting with them and whereas she gets along with everybody so now when we plan things we have to typically reach a compromise about who we invite and that's taught me a lot about compromise and communication.</p>\n",
            "<p>&nbsp;</p>\n",
            "<p>She's a huge animal person. Her house has a level in between the basement and the garage and that entire space is just dedicated to different animals that her family fosters. She currently has a very old and quite large dog named Lucy who happens to not have eyes. She also has two rabbits. Arya is vegan and is the kind of person who refuses to kill any bug in the house. However, if it happens to be a spider, all the rules of the game change.</p>\n",
            "<p>&nbsp;</p>\n",
            "<p>She has excellent taste in clothes. She's always typically the best dressed and most of high school was spent raiding her closet for something to wear. She's was the one friend that did everybody's makeup before dances. Ironically, she took the longest to get ready. She's typically the last one to show because she's always late. Once, she showed up six hours late to an event. Despite being late, she's always the person you can count on to be there and bring the party. Arya is one of the best people I know and that's why I chose to write about her.</p>\n",
            "</div>\n",
            "</body>\n",
            "</html>\n",
            "\n",
            "One of my favorite people is my best friend Arya. She's one of those people\n",
            "that everybody gets along with. She's incredibly kind and patient with people.\n",
            "Her house was where we threw all the parties since her parents were very\n",
            "laidback. If I think back about high school, most of my best memories were at\n",
            "her house. We were the dynamic duo planners of our group. She was the eternal\n",
            "pacifist and is very welcoming. She'll always invite new people to hangouts\n",
            "and introduce to everybody and make sure they feel welcome.\n",
            "\n",
            "\n",
            "\n",
            "I think its important to have a friend who support you but also tells you when\n",
            "you're making a mistake. She's that friend for me. She's always been a huge\n",
            "supporter and also pushed me when I needed it. She's also never been afraid to\n",
            "call me out on my mistakes. In terms of mistakes, one thing that I appreciate\n",
            "a lot about her is that she's always trying to grow from her mistakes and is\n",
            "very introspective about her behavior.\n",
            "\n",
            "\n",
            "\n",
            "We had a fight a few years ago and our friendship became very surface level\n",
            "and strained. We reconnected again a few years later and we've become much\n",
            "strong friends because of it. We balance each other out now. She's a very\n",
            "nostalgic person and sometimes has a tendency to look at the past through\n",
            "rose-tinted lens and as a result that's taught me look at the past with both\n",
            "its highs and lows. I draw boundaries with people that I don't like and have\n",
            "no issue not interacting with them and whereas she gets along with everybody\n",
            "so now when we plan things we have to typically reach a compromise about who\n",
            "we invite and that's taught me a lot about compromise and communication.\n",
            "\n",
            "\n",
            "\n",
            "She's a huge animal person. Her house has a level in between the basement and\n",
            "the garage and that entire space is just dedicated to different animals that\n",
            "her family fosters. She currently has a very old and quite large dog named\n",
            "Lucy who happens to not have eyes. She also has two rabbits. Arya is vegan and\n",
            "is the kind of person who refuses to kill any bug in the house. However, if it\n",
            "happens to be a spider, all the rules of the game change.\n",
            "\n",
            "\n",
            "\n",
            "She has excellent taste in clothes. She's always typically the best dressed\n",
            "and most of high school was spent raiding her closet for something to wear.\n",
            "She's was the one friend that did everybody's makeup before dances.\n",
            "Ironically, she took the longest to get ready. She's typically the last one to\n",
            "show because she's always late. Once, she showed up six hours late to an\n",
            "event. Despite being late, she's always the person you can count on to be\n",
            "there and bring the party. Arya is one of the best people I know and that's\n",
            "why I chose to write about her.\n",
            "\n",
            "\n",
            "[[['One', 'of', 'my', 'favorite', 'people', 'is', 'my', 'best', 'friend', 'Arya', '.'], ['She', \"'s\", 'one', 'of', 'those', 'people', 'that', 'everybody', 'gets', 'along', 'with', '.'], ['She', \"'s\", 'incredibly', 'kind', 'and', 'patient', 'with', 'people', '.'], ['Her', 'house', 'was', 'where', 'we', 'threw', 'all', 'the', 'parties', 'since', 'her', 'parents', 'were', 'very', 'laidback', '.'], ['If', 'I', 'think', 'back', 'about', 'high', 'school', ',', 'most', 'of', 'my', 'best', 'memories', 'were', 'at', 'her', 'house', '.'], ['We', 'were', 'the', 'dynamic', 'duo', 'planners', 'of', 'our', 'group', '.'], ['She', 'was', 'the', 'eternal', 'pacifist', 'and', 'is', 'very', 'welcoming', '.'], ['She', \"'ll\", 'always', 'invite', 'new', 'people', 'to', 'hangouts', 'and', 'introduce', 'to', 'everybody', 'and', 'make', 'sure', 'they', 'feel', 'welcome', '.']], [['I', 'think', 'its', 'important', 'to', 'have', 'a', 'friend', 'who', 'support', 'you', 'but', 'also', 'tells', 'you', 'when', 'you', \"'re\", 'making', 'a', 'mistake', '.'], ['She', \"'s\", 'that', 'friend', 'for', 'me', '.'], ['She', \"'s\", 'always', 'been', 'a', 'huge', 'supporter', 'and', 'also', 'pushed', 'me', 'when', 'I', 'needed', 'it', '.'], ['She', \"'s\", 'also', 'never', 'been', 'afraid', 'to', 'call', 'me', 'out', 'on', 'my', 'mistakes', '.'], ['In', 'terms', 'of', 'mistakes', ',', 'one', 'thing', 'that', 'I', 'appreciate', 'a', 'lot', 'about', 'her', 'is', 'that', 'she', \"'s\", 'always', 'trying', 'to', 'grow', 'from', 'her', 'mistakes', 'and', 'is', 'very', 'introspective', 'about', 'her', 'behavior', '.']], [['We', 'had', 'a', 'fight', 'a', 'few', 'years', 'ago', 'and', 'our', 'friendship', 'became', 'very', 'surface', 'level', 'and', 'strained', '.'], ['We', 'reconnected', 'again', 'a', 'few', 'years', 'later', 'and', 'we', \"'ve\", 'become', 'much', 'strong', 'friends', 'because', 'of', 'it', '.'], ['We', 'balance', 'each', 'other', 'out', 'now', '.'], ['She', \"'s\", 'a', 'very', 'nostalgic', 'person', 'and', 'sometimes', 'has', 'a', 'tendency', 'to', 'look', 'at', 'the', 'past', 'through', 'rose-tinted', 'lens', 'and', 'as', 'a', 'result', 'that', \"'s\", 'taught', 'me', 'look', 'at', 'the', 'past', 'with', 'both', 'its', 'highs', 'and', 'lows', '.'], ['I', 'draw', 'boundaries', 'with', 'people', 'that', 'I', 'do', \"n't\", 'like', 'and', 'have', 'no', 'issue', 'not', 'interacting', 'with', 'them', 'and', 'whereas', 'she', 'gets', 'along', 'with', 'everybody', 'so', 'now', 'when', 'we', 'plan', 'things', 'we', 'have', 'to', 'typically', 'reach', 'a', 'compromise', 'about', 'who', 'we', 'invite', 'and', 'that', \"'s\", 'taught', 'me', 'a', 'lot', 'about', 'compromise', 'and', 'communication', '.']], [['She', \"'s\", 'a', 'huge', 'animal', 'person', '.'], ['Her', 'house', 'has', 'a', 'level', 'in', 'between', 'the', 'basement', 'and', 'the', 'garage', 'and', 'that', 'entire', 'space', 'is', 'just', 'dedicated', 'to', 'different', 'animals', 'that', 'her', 'family', 'fosters', '.'], ['She', 'currently', 'has', 'a', 'very', 'old', 'and', 'quite', 'large', 'dog', 'named', 'Lucy', 'who', 'happens', 'to', 'not', 'have', 'eyes', '.'], ['She', 'also', 'has', 'two', 'rabbits', '.'], ['Arya', 'is', 'vegan', 'and', 'is', 'the', 'kind', 'of', 'person', 'who', 'refuses', 'to', 'kill', 'any', 'bug', 'in', 'the', 'house', '.'], ['However', ',', 'if', 'it', 'happens', 'to', 'be', 'a', 'spider', ',', 'all', 'the', 'rules', 'of', 'the', 'game', 'change', '.']], [['She', 'has', 'excellent', 'taste', 'in', 'clothes', '.'], ['She', \"'s\", 'always', 'typically', 'the', 'best', 'dressed', 'and', 'most', 'of', 'high', 'school', 'was', 'spent', 'raiding', 'her', 'closet', 'for', 'something', 'to', 'wear', '.'], ['She', \"'s\", 'was', 'the', 'one', 'friend', 'that', 'did', 'everybody', \"'s\", 'makeup', 'before', 'dances', '.'], ['Ironically', ',', 'she', 'took', 'the', 'longest', 'to', 'get', 'ready', '.'], ['She', \"'s\", 'typically', 'the', 'last', 'one', 'to', 'show', 'because', 'she', \"'s\", 'always', 'late', '.'], ['Once', ',', 'she', 'showed', 'up', 'six', 'hours', 'late', 'to', 'an', 'event', '.'], ['Despite', 'being', 'late', ',', 'she', \"'s\", 'always', 'the', 'person', 'you', 'can', 'count', 'on', 'to', 'be', 'there', 'and', 'bring', 'the', 'party', '.'], ['Arya', 'is', 'one', 'of', 'the', 'best', 'people', 'I', 'know', 'and', 'that', \"'s\", 'why', 'I', 'chose', 'to', 'write', 'about', 'her', '.']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHNeMvbIsyoK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}